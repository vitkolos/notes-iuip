# Exam

## Introduction

- agent
	- physical or virtual entity, has its own resources, possibly a representation of its environment
	- acts autonomously to satisfy some tendencies (goals) … proactive
	- while taking into account its limited resources, perceptions etc. … reactive
- agent architectures
	- simple reactive agent
	- agent with an internal state
	- goal-based agent
	- utility-based agent
		- in case of conflicting goals or uncertain effects
- properties of environments
	- accessibility
		- are all the relevant aspects of the environment instantly available to the agent? → we don't need to maintain an internal state
	- determinism
		- is the next state completely determined by the current state and the actions selected by the agents?
	- accessible + deterministic → no uncertainty
	- episodic vs. sequential environment
		- do future states depend on past actions? → sequential environment
		- episodic environments are simpler, agents can make reactive decisions and do not need to remember the history (example: old chatbots)
	- static vs. dynamic environment
		- dynamic environment can change during deliberation → we need to think fast (example: autonomous car)
		- semi-dynamic environment does not change during deliberation but the passage of time is important (e.g. performance score)
		- static environment does not change during deliberation, time does not matter
	- discrete vs. continuous
		- discrete environment – limited number of distinct, clearly defined percepts and actions
	- examples
		- chess: accessible, deterministic, sequential, static or semi-dynamic (with a clock), discrete
		- poker: inaccessible, indeterministic, sequential
		- taxi driving: inaccessible, indeterministic, sequential, dynamic, continuous
- MAS applications
	- distributed problem solving
	- agent-based modelling and simulation

## Agent Architectures

- reactive agent vs. cognitive agent
	- cognitive agent can reason about the environment
	- reactive agent can only move randomly and perform some reactive actions
- practical reasoning (Bratman)
	- action-oriented reasoning = process of deciding what to do (not what to believe or what is true)
	- human practical reasoning
		- deliberation – what state of affairs I want to achieve? (output = intentions)
		- means-end reasoning – how do I get there? (output = plans)
	- philosophical foundation: Bratman's theory of practical reasoning
	- intentions in practical reasoning
		- effort and resources put into achieving intention
		- no conflict: intentions serve as a filter for adopting new intentions
		- success tracking + persistance after failure
		- possible – intentions are believed to be possible
		- feasibility – not believed to never be reached
		- under some circumstances, agents believe they will reach their intention
		- agents need not intend all the expected side effects of their intentions
- BDI architecture
	- beliefs
		- representation of the world
		- possible incorrect and incomplete
		- but consistent/rational
	- desires
		- our goal (ideal states)
	- intentions
		- how to achieve the goal
		- intentions lead to actions
		- in a dynamic environment, we need to reconsider the intentions
			- constraints: resources, time (in real-time envornments)
			- two extreme strategies
				- never reconsider
				- reconsider constantly
			- strategies
				- blind commitment – only reconsider after success / total fail (all plans have failed)
				- single-minded commitment – also reconsider when intention becomes impossible
				- open-minded commitment – explicit meta-level controller that decides if intentions should be reconsidered
	- BDI goal-plan diagram (tree)
		- list of all plans we can use to achieve the goal
		- each plan consists of several atomic actions
		- example: gold miners
			- goal: earn money
			- plans: sell gold | other job
			- how to sell gold: have gold & go to market & sell
			- how to have gold: steal gold | pick gold
		- on every level of the tree, we switch between AND and OR
	- BDI pros & cons
		- grounded in philosophy, formal logic
		- high level of abstraction, captures folk psychology
		- explains why an agent does something
		- extensible, flexible, robust, failure recovery
		- dedicated programming frameworks (example: GAMA)
		- but complex, may be computationally slow
- reflex architecture
	- stimulus-action rules
	- directly triggered by stimuli in the environment
	- less complex than symbolic AI
	- example: ants
		- go back to nest = follow pheromones the other way
	- finite-state machines
	- subsumption architecture
		- hierarchy of competence modules (CM)
		- each module is responsible for a concrete, clearly defined, simple task
		- all modules operate in parallel
		- rather simple rule-like structure
		- lower layers can inhibit higher layers
			- by suppressing input signals or inhibiting output signals
	- example: Mars explorer (Steels)
		- highest priority: obstacle avoidance
		- drop carried samples at base
		- return to base when carrying sample
		- collect found samples
		- lowest priority: exploration
	- how can reactive agent locate the base?
		- gradient field – the base emits radio signal
	- how can robots communicate together?
		- we want to tell the other robots where the clusters are
		- we use “feromones” – stigmergy (indirect communication through environment)
			- robots drop radioactive crumbs
			- other robots need to pick them to make the trace “fade away”
		- https://nausikaa.net/wp-content/uploads/2022/09/steels-mars-explorer-02.html
	- for reactive agents, the environment has to be accessible
- hybrid architectures
	- reactive and deliberative layers
		- obstacle avoidance can be reactive
	- how to handle interaction between layers?
		- horizontal layers – each layer produces output, they are merged
		- vertical layers – layers pass inputs in one direction and outputs in the other
	- Ferguson – Touring Machines
		- three control layers: reactive, planning, modelling

## Game Theory

- applications
	- biology – survival of the fittest (winners)
	- political sciences – military strategy, Cold War
- typology of games
	- zero-sum games
	- cooperative vs. competitive
	- simultaneous vs. sequential
	- information available – complete information, incomplete information (Bayesian games), imperfect information
- examples
	- shifumi (rock-paper-scissors)
	- chicken game
- notation
	- players $i,j$
	- results $W=\set{w_1,\dots,w_n}$
- agents maximize utility
	- absolute values of utility does not convey much meaning, we are more interested in relative comparison (which action has more utility for the agent)
- simplified environment model
	- possible actions $Ac=\set{C,D}$
		- cooperate $(C)$, defect $(D)$
	- the environment is…
		- sensitive – if each combination of actions leads to a different result
		- insensitive – no agent has any influence on the result
		- controlled – if one player can influence the result
	- utility functions – map results to utilities
- representation
	- extensive form – decision tree
	- strategic form – matrix
- notions
	- pure or mixed strategy
	- equilibria
		- Nash equilibrium – no agent can improve his payoff by changing his own strategy
		- Bayesian equilibrium – extension of NE to incomplete information games
	- dominant strategy – $s_1$ dominates $s_2$ if $s_1$ always leads to a better utility (regardles of the other players' strategies)
	- Pareto optimality – outcome cannot be improved without hurting at least one player
	- social vs. individual benefit
- prisoners' dilemma
	- happens in every situation where $T\gt R\gt P\gt S$
	- $T$ … temptation (successful betrayal)
	- $R$ … reward (we both cooperated)
	- $P$ … punishment
	- $S$ … sucker (I was betrayed)
- how can we establish cooperation in multi-agent systems?
	- iterated prisoners' dilemma
	- Axelrod's tournament
		- winner – tit for tat
- tragedy of the commons, free riders
	- shared resources
	- benefits are individual, costs are shared
- humans are not always economically rational

## Communication

- Shannon's communication model
	- source
	- transmitter (message → signal)
	- channel (transmits signals)
	- receiver (signal → message)
	- destination
- Berlo's communication model
	- sender
	- message
	- channel
	- receiver
- types
	- point to point × broadcast
		- when broadcasting, sender does not (might not) know the receiver
	- broker – ensures that the message is delivered to everyone interested (reduces broadcast to point-to-point)
	- propagation in environment
- message meaning
	- intentional – sender intends the meaning of the message
	- incident – receiver interprets (gives a meaning to the message)
- 7 steps
	- speaker
		- intention – what information is the speaker trying to communicate
		- generation – generate the message
		- synthesis – send the message (say it…)
	- hearer
		- perception – receive the signal
		- analysis – infer possible meanings
		- disambiguation – try to choose the most probable meaning
		- incorporation – decide to believe the communicated information (or not)
- communication problems
	- technical – message does not arrive at all
	- semantic – hearer does not understand the meaning
	- efficiency – message does not have the intended effect (the hearer chooses not to believe it…)
- misunderstading, potential meanings
	- M1 … original intention (meaning)
	- M2 … meaning that the hearer infers
	- M3 … what the speaker believes that the hearer inferred
- speech acts theory: Austin
	- locutionary act – utterance
	- illocutionary act – intent
	- perlocutionary act – result
- Searle: categories of illocutionary acts
	- assertives – commit the speaker to the truth of the proposition
	- commissives – commit the speaker to a future action
	- directives – cause the hearer to take an action
	- declaratives – change the reality in accord with the content of the declaration
	- expressives – express the speaker's attitudes/emotions
- Vanderveken
	- decomposition into illocutionary force (F) and propositional content (P)
		- F … general intent (inform, ask-to-do, request, answer, …)
		- P … specific content
	- success and satisfaction conditions
		- success if the hearer recognizes the intention
		- satisfaction if the speaker's intention is achieved
- we need to know the preconditions and the effects of each speech act
	- we also need boolean conditions to see if a speech act is successful and/or satisfied
- agent communication languages
	- human languages are ambiguous → agents use interaction languages
	- approaches
		- mentalist(ic) approach
			- based on beliefs
			- FIPA-ACL
			- some strong assumptions/hypotheses – agents are sincere and willing to cooperate
		- social approach
			- based on commitment
			- commitments are public
			- are there two contradictory commitments?
		- public approach – based on grounding
		- deontic approach – based on norms
- interaction protocols
	- shown on KQML
		- direct (point to point)
		- through a matchmaker
		- through a broker
		- through a feeder
	- example: typical request protocol (agents are taking turns)
		- request → accept, refuse, or modify
		- accept → inform-done or inform-failure
		- modify → accept, refuse, or modify
	- contract net
		- “I need help for a task”
		- 5 stages: recognition (of my own problem), announcement, bidding, awarding, expediting
		- bidding, contracts
	- dependence based coalition (DBC), social reasoning
- programming communication
	- keyword-based chatbots
	- logical programming
	- LLMs
- simulating communication
	- with neighbours – turtles move and message neighboring turtles
	- with acquaintances – every turtle has some links
- trust
	- unknown agent → based on reputation or stereotypes
	- known agent → based on interaction history
	- approaches: economical (maximizing utility) × social (based on BDI)
	- social trust theory – Castelfranchi, Falcone

## Modelling

- agent-based modelling and simulation
	- model – simplified abstraction of reality
	- macro patterns emerge from individual decisions
- 7 goals of simulation (Axelrod)
	- prediction – e.g. weather
		- predict potential outputs resulting from known complex inputs and assumptions about mechanisms
	- task performance
		- we want to mimic a human performing the task (human's perception, decision making, social interaction)
	- training – e.g. flight simulator
		- train people by providing a reasonably accurate and dynamic interactive representation of a given environment
	- entertainment – e.g. video games, movies
		- imaginary virtual world, for amusement
	- education
		- users can learn what happens if they do *this* or *that* in the simulation
	- proof – e.g. Conway's game of life, Shelling's segregation model
		- prove existence/conjecture
	- discovery
		- discover new knowledge, principle, relationships
- social simulation
	- model should be valid (faithful to reality)
- how to build a model
	- preparation
		- define research question
		- formulate hypothesis
		- define (input) parameters and output indicators
		- find relevant literature/insights describing the modelled behaviour
		- define rules of the system
		- formulate what is important (and what is not) – start simple
	- who are the agents?
		- attributes, actions, architecture, decision making, base theory (psychology etc.)
	- what is the environment?
		- space, time
	- simulator
		- inputs
		- outputs
		- what we show?
	- implementation
		- NetLogo, GAMA
- examples
	- boid simulation in movies
	- Game of Life
	- Schelling's segregation model
	- crowd simulation
	- traffic simulation
	- urban planning
		- Acteur project, multi-level decision-making
			- strategical – establish list of destinations and try to reach them
			- tactical – adjust plans to implement strategy
				- ordinary situation – adjust to traffic, choose best trajectory, less populated roads, …
				- extraordinary situation – escaper (flee danger), bystander, random wanderer, road runner (less congested), sheep (follow crowd), …
			- operational
		- HIANIC project
			- shared space (cars, pedestrians, bikes)
			- autonomous car navigation
		- Switch project
			- car → bike?
			- 4 mobility modes (walk, bike, bus, car)
			- 6 criteria (comfort, ecology, price, simplicity, safety, time)
				- every agent has priorities
			- decision model with habits
				- with some probability, we rationally reevaluate (if the context has changed – price of gas went up…)
				- otherwise, we stick to our habit
	- evacuation modelling, crisis management
		- evacuation – zigzag stairs may be better
		- flood risk management, communication
		- epidemics
		- earthquake
			- Solace – testing the role of social attachment
		- you need realistic cognitive agents
			- take human factors into account
	- not all models require the same level of realism
		- entertainment models do not have to be that much realistic
- human factors
	- emotions, empathy, mood, personality
	- trust, moral values, ethics
	- cognitive biases
	- motivation, engagement
	- memory, attention, distraction, tiredness, focus, stress
	- social links, attachment, altruism, cohesion
	- we need to simulate emotions
		- BDI logical model of emotions
		- book: The Cognitive Structure of Emotions
		- example: distress … agent believes that $\varphi$, but desires $\neg\varphi$
	- biases – confirmation bias, …

## Modelling COVID-19

- types of models
	- mathematical models
		- people in 3 compartments
			- susceptibles
			- infected
			- recovered
		- parameters
			- $\mu$ … birth/death rate (supposed equal)
			- $\beta$ … contamination rate
			- $\gamma$ … recovery rate
	- agent-based models
		- agents can be heterogenous (individual attributes, individual choices)
		- more detailed, better explainibility
		- slower, more complex
		- need individual behaviour data
- data collection
	- place of death
		- people dying in hospitals (in cities), residence address not always available
		- leads to overestimating the gravity of the situation
	- mortality = deaths / cases
		- how to count the deaths? (what if covid was probably not the real cause of death?)
		- how to count the cases? (there are asymptomatic cases…)
- CovPrehension project
	- modelling the spread
		- simplified, 1 contact leads to infection
		- we can calibrate the model based on the reproduction rate
		- we can introduce physical distancing – but in reality it's not always feasible
		- some people cannot / don't want to respect distancing
	- one model not enough to make important political decisions
	- collective immunity
	- who should be tested first?
		- testing symptomatic people is not enough to estimate the number of infected people
- designing models
	- make your hypotheses clear
	- choose useful attributes, keep agents simple
	- choose a limited number of parameters, should have a visible impact on the output
	- output indicators should be measurable from the model and useful to answer the question

## Agent Control

- basic types
	- procedural – limited
	- learned – needs data, prone to biases
	- control by planning – goal oriented behavior, BDI architecture
- difficulties of planning
	- environment – dynamic, partially observable, continuous
	- perception errors
	- goals – reach a state (partial satisfaction?) / maximize a utility
	- actions – duration, uncertainty, concurrency
	- other agents – dynamism, conflict, cooperation
- classical planning problem
	- many simplifying assumptions
	- unique (known) initial state
	- actions
		- instantaneous
		- deterministic (→ accurate prediction of next state)
		- one at a time
	- static environment, no other agents
	- → goal-oriented behaviour
- model of the problem – we need a formal description of all the following elements
	- initial state
	- list of goals
	- set of actions – with their preconditions and effects
- one approach: situation calculus
	- John McCarthy
	- basic elements
		- actions
		- fluents – describe the state of the world
		- situations – sequence of (past) actions
	- domain then consists of
		- world state descriptions (fluents)
		- actions (one precondition axiom per action + effect axioms)
		- successor state axioms (one per fluent; how fluents change over time)
	- how situations work
		- let's say we are in situation $S$
		- then $S'=\mathrm{Do}(\mathrm{action}(\dots),S)$ is the situation after performing *action*
		- so the situations store the history
	- action preconditions and effects
		- *Poss*
		- block world example of precondition: Poss(move(x,y,z),S) <-> clear(x,S) & on(x,y,S) & clear(z,S)
	- frame problem
		- successor state axioms solve it
- definition: plan
	- set of plan steps
	- set of ordering constraints (timing of steps) – what needs to happen before what; total or partial order
	- set of variables
	- set of causal links between steps
	- several types of plans: linear, non-linear, hierarchical
	- examples: recipe, itinerary, hanoi towers
- planning algorithms
	- properties: soundness (produces only correct solutions), completeness (produces all existing solutions), optimality (finds the best solution first), speed
	- we can use graph search algorithms: DFS (possible endless loop if tree-search used on a graph), BFS, A\*
		- *expand*, *insert*
		- forward chaining – what can we do at this point?
		- backward chaining – what can lead us to our goal?
	- linear planning
		- means-end analysis (Newell) – tries to reduce the difference between the current state and the goal
		- STRIPS (Fikes, Nilsson) – implements means-end analysis
		- Sussman anomaly – if we divide the goal (conjuction) into subgoals, we may get suboptimal plans
		- linear plan = sequence of actions
		- linear planning – summary
			- state space
			- operators – transitions between states
			- test function (is goal reached?)
			- path cost (how many steps)
			- progressive × regressive (forward or backward chaining)
		- advantages
			- reduced search space – goals solved one at a time
				- advantageous if goals are independent (but they may often conflict!)
			- sound – only finds correct plans
		- disadvantages
			- may produce suboptimal solutions
			- incomplete
	- non-linear planning
		- considers all goals at the same time
		- more complex
		- may be parallelized

## Coordination

- coordination of multiple agents
	- local decision, global goal, anticipate other agents' behavior
	- multiplication and specialization of agents
		- increase effectiveness (redundancy, economy of scale)
		- but also introduce some challenges (communication, making rational choices, planning, cooperation, coordination, negotiation)
	- additional information processing we need to perform when multiple agents pursue goals (if there was only a single agent, it would not perform these processing steps)
	- managing dependencies between agents
	- purpose of coordination … increase effectiveness with increasing number of agents
- reflex agents – reactive coordination
	- no planning, no direct communication with other agents
	- methods for reactive coordination: stimergy (pheromones, radioactive crumbs), force fields, behavior rules (boids)
- cognitive coordination
	- planning, communication (e.g. ContractNet), organization (e.g. form coalitions)
	- self-interested vs. benevolent agents
		- problem-solving in benevolent system … cooperative distributed problem solving
		- self-interested agents → game theory
	- are the goals compatible, resources sufficient, and skills (of individual robots) sufficient? (as described by Ferber)
		- compatible goals & sufficient resources & sufficient skills → disinterest, independance
		- compatible goals (but missing skills or resources or both) → cooperation
		- incompatible goals → antagonism
	- coodination mechanisms
		- centralized × decentralized
		- static (rule-based) × dynamic
		- implicit (altering the environment to gradually solve the problem) × explicit (what to do and when)
- coordinated distributed problem solving (CDPS)
	- divide and conquer
	- sub-problems are easier to solve, can be solve in parallel
	- four steps: decomposition, task allocation, local solving, conflict solving
	- a good decomposition allows agents to work in parallel
	- positive × negative interaction
		- positive interaction – action helps achieving several necessary facts
		- negative interaction – achieving a fact deletes other necessary facts
	- homogeneous (all have the same skills) × heterogeneous agents
		- task allocation for homogeneous agents is easier
	- local solving – each agent generates its sub-plan
	- conflict solving – sharing information, synchronizing actions and resource access
		- it's easier to solve conflicts between sub-plans than to design a global plan based on global constraints
- task allocation protocols
	- agents can form temporary alliances – coalitions
	- ”who can I work with?“
	- if agents don't know skills of others → ContractNet
	- if agents know skills of others and can reason about them → dependence-based coalitions (DBC)
- dependence-based coalitions
	- Castelfranchi
	- goal-dependence
		- agent $i$ depends on agent $j$ for a given goal $g$ w.r.t. a set of plans $P$ if…
			- $i$ has $g$ in its set of goals and has no feasible plan achieving $g$
			- but there exists a plan $p$ achieving $g$
			- and $j$ has an action $a$ in its set of actions, s.t. $a\in p$
			- and $i$ does not have such action
		- OR-dependence – any of these agents can help me
		- AND-dependence – I need all these agents to help me in order to achieve my goal
	- principles of DBC: non-benevolence (agents can refuse to cooperate with others), sincerity, self-knowledge, consistency (agents don't maintain contradictory beliefs about others)
	- diagram in slides
		- plans $p111,p112,p18$
		- agent 1 can perform $a1$, does not need help
		- $p111$ … AND-dependence
		- $a2$ … OR-dependence
	- situations for agent $i$ and goal $g$
		- NG … agent $i$ does not have goal $g$
		- NP … has $g$ but no plan
		- AUT … has $g$ and autonomous plan
		- DEP … has $g$ and every plan is action-dependent
	- situations for agents $i,j$ and a goal $g$ (according to $i$'s plans and $j$'s plans that $i$ knows about)
		- independence – $i$ does not need $j$ to achieve $g$
		- unilateral dependence – $i$ needs $j$ for $g$ and $j$ does not need $i$ (for any goals)
		- mutual dependence – $i$ needs $j$ and $j$ needs $i$ to achieve $g$
		- locally believed mutual dependence – $i$ believes there's a mutual dependence and also believes that $j$ does not believe so
		- mutually believed mutual dependence – $i$ believes that both $i,j$ believe there's a mutual dependence
	- situations for $i,j$ and $g,g'$
		- reciprocal dependence – $i$ needs $j$ for $g$ and $j$ needs $i$ for $g'$
		- locally believed reciprocal dependence
		- mutually believed reciprocal dependence
	- choice of partner
		- it's better to choose a partner with a mutual dependence than a reciprocal dependence (no reciprocation problem)
		- it's better to choose a partner with a mutually believed dependence than with a locally believed dependence (no problem of convincing the partner)
	- social reasoning (steps)
		- agent chooses a goal to achieve
		- agent chooses a plan to execute
		- agent checks its goal situation
		- agent executes the plan (if autonomous) or chooses a partner to form a coalition
- multi-agent planning
	- distributed STRIPS

## Social Choice

- general problem
	- set of options
	- set of agents expressing opinions on that options
	- how to select an option?
- fair allocation (division) of resources
	- example: Pleiades satellites constellation
	- the division (allocation) problem
		- inputs: finite set of agents with preferences/demands (numerical or ordinal), common limited resource (divisible or indivisible), set of constraints, optimization criterion
		- output: allocation
	- central principle – every agent knows how they value the resource, they should make the division themselves (there should be no central bottleneck)
	- the resource can be heterogeneous and divisible (example: cake)
	- utility function
		- $u:2^X\to [0,1]$
		- normalized, so that $u(\emptyset)=0$ and $u(X)=1$
		- usually additive so $u(A)+u(B)=u(A+B)$ for $A\cap B=\emptyset$
	- Pareto dominance
	- Pareto efficiency – quite weak criterion (giving the cake to one agent is efficient but not necessarily fair)
- what is fair?
	- no objective value of each item
	- two ways to define a good division
		- maximizing social welfare (collective utility)
		- maximizing individual criteria
	- collective utility (social welfare) function can be defined in different ways
		- utilitartian … maximizing total utility
		- egalitarian … maximizing the minimum utility
		- Nash collective utility function … maximizing the product of utilities
	- individual fairness criteria
		- envy-free
			- there is no other agent whose bundle (share) would the agent want more
			- does not always exist
		- exact
			- all agents agree about the value of all shares
		- proportional
			- if each agent gets at least their due share according to their own value function
			- if they have strictly more → super proportional
		- equitable
			- agents feel the same hapiness
			- hard to verify
	- price of fairness – fair division may be less efficient than some other division (according to economic welfare)
- allocation approaches
	- centralized allocation
	- distributed allocation
		- start with random allocation
		- let the agents negotiate
	- sequential allocation
		- agents take turns
		- problem: find a good sequence
		- knowing preferences of other agents can be used to manipulate the result
	- example: cut & choose method
		- first agent cuts cake into equal parts (subjectively)
		- second agent chooses which part they prefer
		- is proportional and envy-free
		- adaptations for more agents
- voting theory
	- formal framework
		- $n$ voters, finite set $X$ of $m$ alternatives
		- each voter expresses a preference = linear order over $X$
		- profile $R$ fixes one preference for each voter
		- voting rule / social choice function is a function $F$ mapping any given profile to a non-empty set of winning alternatives
			- $F:L(X)^n\to 2^X\setminus\set{\emptyset}$
		- $F$ is *resolute* if there is always a unique winner … $\forall R:|F(R)|=1$
	- voters need to express strict preferences
	- some nonstandard methods don't fit into this framework
	- rules
		- plurality rule – wins the one who ranks first the most
		- Borda rule – voters assign points in order (e.g. 4, 3, 2, 1, 0 for $m=5$)
		- plurality rule with run-off (two rounds)
	- we want to incentivize voters to be truthful
- single transferable vote (STV)
	- generalization of the plurality with runoff
	- collect preferences, eliminate loser (with the least top votes), recalculate preferences (without the loser), eliminate the next loser, …
	- if someone gets over 50 % in the process, this is the winner
	- used in some countries
	- no-show paradox
		- it may be better to abstain than to vote for your favorite candidate
		- → participation criterion
			- addition of ballot $A\succ B$ should not worsen the position of $A$
- positional scoring rule (PSR)
	- generalization of Borda rule
	- voters assign scores to candidates
	- Condorcet principle
		- the total winner should win in every pairwise majority contest
	- PSR violates Condorcet principle
		- Condorcet principle is incompatible with participation criterion
- utilitarian vs. egalitarian rule
	- utilitarian rule – pick the alternative maximizing the sum of the utilities of all individuals
	- egalitarian rule – pick the alternative maximizing the minimum utility of all individuals
- judgement aggregation problem
	- consolidating individual beliefs
	- doctrinal paradox
		- we cannot just use majority vote to consolidate interdependent statements (it may lead to inconsistency even if the individual agents are consistent)

## Creating Models

0. define the research question
	- then, we can create a model and its implementation (i.e. simulator)
1. define the agents
	- agents: attributes, actions, interaction, architecture/complexity
2. use data to initialize attributes
	- based on the desired level or realism
3. define the environment
	- time and space scale
	- map type
4. implement the simulator
	- input parameters, output indicators & visualizations
	- which scenarios to explore?
	- which platform to use?
