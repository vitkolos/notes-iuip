# Lecture

- computer vision
	- mimic human vision
	- extract measurable information
	- identify objects
	- fix/improve images to improve their interpretation
- image processing vs. computer vision
	- processing: reasoning focused on the image, pixels, pixel groups
	- vision: focuses on the knowledge the image brings from a real scene
- difficult problem
	- the goal of computer vision is not to mimic human vision but to build systems that extract information
	- computer vision is an inverse of the synthesis problem
		- projection is fundamentally ambiguous – we are losing information (depth, size, occlusions)
		- in general it's an ill-posed problem: no unique solution for a given observation, ambiguous solutions, incomplete data (example: scale of the observed scene, toy car instead of normal car)
		- need of injecting a priori knowledge and regularization (example: penalize non-smooth solutions)
	- from noisy observations, we estimate the parameters of a model
	- a priori knowledge: physics, geometry, semantics
	- example: by counting visible wheels of the car, we can tell the position of the camera
	- desirable characteristics
		- robustness – be able to identify observation noise/errors (have plan B in case of error)
		- speed
		- precision
		- generality – the algorithm should be generic; the pool of situations that it can handle should be large enough
- main vision problems
	- calibration – where is the camera, which model, …
	- segmentation – which parts of the image belong together
	- detection – what is interesting/salient in the image
	- reconstruction – where are the objects (position, shape, 3D surface)
	- tracking – which motion is present in the image, how do objects move
	- recognition – what do we see, semantics
- image
	- 2D signal, depicts a 3D scene
	- matrix of values that represent a signal
	- has semantic information
- light
	- plays a fundamental role in 3D perception
	- no light → no image
	- diffuse reflection, shadow, specular reflection
	- wavelength, spectrums
		- solar spectrum: almost continuous spectrum, some wavelengths are stronger
		- white light: continuous spectrum (energy evenly distributed)
		- sodium vapor lamp: only yellow → red car appears dark
	- what happens when a light ray hits the surface
		- absorption (black surface)
		- reflection, refraction (mirror, reflector)
		- diffusion (milk)
		- fluorescence
		- transmission and emission (human skin)
	- most surfaces can be approximated by simple models
	- first simplified hypothesis
		- no fluorescent surfaces
		- “cold” objects (they don't emit light)
		- all light emitted from a surface point is formed solely by the light arriving there
	- standard model: BRDF
		- bi-directional reflectance distribution function
		- models the ratio of energy for each wavelength $\lambda$
			- incoming from direction $\hat v_i$
			- emitted towards direction $\hat v_r$
			- $\hat n$ … normal vector
		- reciprocity
		- isotropy
		- energy corresponds to an integral (we can use a discrete sum)
		- Lambert assumption
			- diffuse surface: uniform in all directions (paper, milk, matt paint)
			- the BRDF is a constant function $f_d(\hat v_i,\hat v_r,\hat n,\lambda)=f_d(\lambda)$
		- specular material, central lobe on $\hat s_i$
			- Phong … $f_s(\theta_s,\lambda)=k_s(\lambda)\cos ^{k_e}\theta_s$
			- Torrence-Sparrow
		- di-chromatic
			- diffuse + specular
- pipeline in a digital camera
	- to get RAW
		- optics → aperture → shutter → sensor → gain → A/D
	- to get JPEG from RAW
		- demosaic → sharpen → white balance → gamma/curve → compress
	- optical role: isolate the light rays (from one particular part of the scene)
	- we can model a complicated system of lenses using just one lens
	- perfect lens: hypothesis
		- a point in the scene corresponds to a point in an image
		- this is not true, there are artifacts
	- chromatic artifacts (fringing)
		- diffraction – wavelength dependent
	- vignetting … border of the image is darker
	- geometric distortion (for wide-angle cameras)
	- CCD sensor, CMOS sensor
	- rolling shutter
		- lines of the sensor are discharged sequentially → moving objects can be distorted
		- usually in cheap CMOS
	- color spaces
		- RGB … additive
		- CMY … subtractive
	- color perception
		- retina
		- fovea
		- rods – achromatic perception of lights, pigmentation (rhodopsin) is sensitive to all visible spectrum (peak on green)
		- cones – color perception
		- mantis shrimp has the most complex visual system ever discovered
	- color perception in a camera
		- deviation/dispersion prism
			- 3 CCD sensors
			- precise alignment, high quality filter
			- expensive
		- Bayer filter
			- individual (plastic) filter for each pixel (RGGB, RGCB)
			- to get colors in each pixel, we interpolate (integrate over spectrums)
	- sensor artifacts
		- noise: salt and pepper, thermic noise (as the camera heats up)
		- aliasing
	- gamma correction – adjusting brightness to match human perception
	- JPEG compression artifacts
	- color models
		- RGB – additive model, mixing wavelengths
		- CMY – subtractive model, mixing pigments (“subtracting” wavelengths)
		- HSV or HSB – hue (“intrinsic color”), saturation, value
			- useful for artists
		- YUV, YIQ – 1 luma component + 2 chroma components
			- for TV broadcasting, luminance can be used for black & white TV
	- application: robotic detection
		- we can detect an object based on its color (ignoring luminance or value)

## Filters, Contours, Segmentation

- characterization of an image
- contours, regions (between the contours) – duality
- noise filtering
	- “how to distinguish between information and noise”
	- most simple and adopted model: Gaussian noise
	- convolution with a low-band Gaussian filter
		- theorem: derivative of convolution = convolution of derivative
	- edge notion is binary – we need a threshold
	- average over neighboring pixels … mean filter
		- but Gaussian filter works better – the neighboring pixels have less effect
- edges … fundamental for human perception
	- how to detect them?
		- smoothing → maximum (using first derivative) or zero-crossing (second derivative)
	- edges' properties
		- contrast
		- orientation
	- edge descriptors
		- normal – unit vector, direction of maximal (intensity) change
			- direction – perp. to normal
		- position
		- intensity
	- image is not a continuous function – we approximate it using Taylor expansion (just using a plane)
	- finite differences
	- gradient operator: Sobel
		- modern neural networks tend to learn this filter to process data :)
		- approximates horizontal ($\frac{\partial I}{\partial x}$) and vertical ($\frac{\partial I}{\partial y}$) gradients
	- scaling of the filtering kernel influences the detected contours
	- another operator: Laplace
	- Laplacian of Gaussian (LoG) … we apply Laplacian on the Gaussian and then convolute it with the image
		- reverse Mexican hat
	- contour extraction (Canny)
		- we only take one pixel with the highest value in the direction of the gradient
- line detection – Hough transform
	- how to express a line
		- $ax+by+c=0$
		- or we can use polar coordinates: $(r,\theta)$
			- $r$ … distance from the origin
			- $\theta$ … slope (kind of)
	- let's vote!
		- each observation adds one point to each line going through it – to the pair $(r,\theta)$
		- we have a restricted set of parameters → it works
- segmentation
	- Gestalt theory
	- region – group of pixels with similar properties
	- we need a similarity measure
		- distance to the mean of the region
		- Mahalanobis distance
	- similarity in color space
		- k-means
		- Gaussian mixture models
			- probability of a point belonging to a cluster
			- probability to observe a point is a mixture of Gaussians
			- expectation maximization is used to solve it
				- E: knowing the current estimates of blobs, we compute the probabilities of the points belonging to them
				- M: we recompute the blob parameters to maximize likelihood
			- some advantages
			- still problems: we still need to choose $k$; it is sensitive to initialization; the generative model (shape of the blobs) need to be chosen
	- superpixels
		- we need to group based on both color and spatial proximity
		- segmentation using a graph partitioning strategy
			- one pixel → one vertex
			- edges to other pixels
			- remove the edges of low similarity
				- based on distance, intensity, color, …
	- Grabcut – interactive method
	- Meta AI: Segment Anything
		- trained using superpixels
	- deep clustering
		- they trained encoder and decoder
		- k-means in the latent (embedding) space

## Interest Points

- motivation
	- 2D tracking (motion)
	- find correspondances
	- recognition
- segmentation is usually avoided – it is a source of errors
	- finding interest points is more robust
- desired properties
- Moravec's detector
	- small window
- Harris detector
	- Taylor expansion
	- bilinear form
	- eigenvalues
		- hard to compute → we use $R$ instead
- we don't want too many interest points, different strategies
- Harris detector properties
	- rotation invariance
	- invariance to intensity shift, partial invariance to intensity scaling
	- no invariance to scale changes
		- we consider circular regions of different sizes on a point
		- but how to choose them?
		- LoG, we can also use difference of Gaussians which is similar (but more efficient to compute)
		- image pyramid
- how to match detected points?
	- we need descriptors – should be invariant and discriminant
	- Harris: use eigen values? not so discriminant
	- windowed approaches – not so invariant
	- multi-scale oriented patches (MOPS)
	- scale invariant feature transform (SIFT)
	- matching repetitive patterns – possible approaches
		- two-way matching (are we the best match for our best match?)
		- for each match, we get a confidence score
- matching – fast techniques

## Deep Learning

- non-linear trasformations
- we compose simple functions and create complex functions
- CNNs
- https://chriswolfvision.medium.com/what-is-translation-equivariance-and-why-do-we-use-convolutions-to-get-it-6f18139d4c59
- pooling to enforce invariances
	- we want translation equivariance (detect the object wherever it is on the image)
- AlexNet
- http://www.incompleteideas.net/IncIdeas/BitterLesson.html
- exercise in slides
- methodology – in general
	- state the problem you want to solve
		- classification, regression, segmentation
		- state input and output
		- narrow down the application domain
	- choose a “vanilla” architecture
	- choose the losses
	- evaluate properly (cross validation, train/validation sets)
	- be aware of (and explore) the biases in the datasets
	- push the boundaries
- paper session
	- plenoptic function, early vision
		- systematic framework capturing visual information
		- plenoptic function – 3D movie, wavelength
			- output – intensity
		- extraction of information … derivatives
		- “periodic table”
		- “blobs” – convolutional filters for extracting information
	- focused plenoptic camera
	- depth estimation
		- we want
			- simultaneous detection and depth estimation
			- local method to detect rays
			- method invariant to object size and depth
			- …
		- ray gaussian kernel
		- lisad
			- operator activation
		- ray $\neq$ light
			- we stack pictures taken from different angles
			- on the cut, there are “rays” → we can get depth (thanks to parallax)
				- if we move, the objects in the foreground shift
				- the objects in the background not that much
		- baseline
	- spray-on optics
		- drop extraction and simulation
			- + remove distortion
		- match the images
		- what is the final resolution?
		- manual droplet segmentation
		- better image quality with more droplets
	- takeaways
		- coach: suggest improvements
		- advocate: don't be too generic
		- using videos
		- how to get real data? ground truth?
			- usually, we need both synthetic and real data