# Lecture

- supervised ML
	- training data $(x^i,y^i)$
	- given a new $x^0$, provide a prediction $\tilde y^0$
	- consider a class $\varphi$ of functions
	- find a function $f_\theta\in\varphi$ (find parameters $\theta$) that minimizes some loss function
		- $\mathrm{argmin}_\theta E(\theta)$
		- where $E(\theta):=\sum_i^n\ell(f_\theta(x^i),y^i)$
- gradient descent
	- $\theta_{t+1}\leftarrow\theta_t-\eta_t\cdot\nabla E(\theta_t)$
	- works well for convex functions $E$
	- doesn't work well for non-convex $E$
	- in practice, works well for neural networks
- multi-layer neural networks
	- $x_{\ell+1}=\sigma(W_\ell x_\ell+b_\ell)$
	- $\sigma$ … activation function (sigmoid, ReLU, …)
	- $W$ … weight matrix
	- $b$ … bias vector
- for two layers
	- $f_\theta(x)=\frac1n\sum_{k=1}^n u_k\cdot\sigma(\braket{v_k,x}+b_k)$
		- $n$ … number of neurons in the hidden layer
	- weights $\theta=(u_k,v_k,b_k)_{k=1}^n$
- Universal approximation theorem (Cybenko '89, Hornik '89)
	- any continuous function $f:\mathbb R^d\to\mathbb R$ can be approximated by a 2-layer neural network
	- (Baron '93): $\|g-f_\theta\|\leq \frac{O(R\|g\|)}{\sqrt n}$
	- conclusion: a large 2-layer neural network is good enough
	- Bach and Chizat '18: gradient descent (flow) converges to the global minimum for 2-layer neural networks
		- we don't know how long it could take
- very deep networks (CNN, Transformer, …)
	- residual networks (ResNet '16)
		- $x_{\ell+1}=x_\ell+\frac 1L U_\ell^T\cdot\sigma(V_\ell x_\ell+b_\ell)$
		- $U_\ell,V_\ell\in\mathbb R^{n\times d}$ … weight matrices
			- $n$ … number of neurons
		- $x_0\xrightarrow{f_\theta} x_L$
		- $L$ layers
	- neural differential equation
		- for $L\to\infty$
			- $\frac{dx_s}{dt}=U_s^T\cdot\sigma(V_sx_s+b_s)$
			- where $s\in(0,1)$
		- theorem (2024): if the starting point is close enough to a local minimum, then the flow induced by $\frac{dx_s}{dt}$ converges to the local minimum
- attention, transformers, BERT
	- modeling sequences with RNN
	- attention in neural networks – originally in computer vision
	- problems of RNNs: long range dependencies, gradient vanishing (and exploding), large number of training steps, no parallel computation
		- transformers solve all four of these
	- complexity per layer
		- for self-attention … $O(n^2d)$
	- BERT – just the encoder
	- how to process long sequence
		- divide into smaller chunks
		- optimize (attention)
	- recommended watch: Transformers, the tech behind LLMs (3Blue1Brown)
	- what happens in the first layer
		- $E^{(1)}=\mathrm{Norm}(E^{(1)}_1+\mathrm{FFN}(E^{(1)}_1))$
		- $E^{(1)}_1=\mathrm{Norm}(E^{(0)}+\mathrm{MATL}(E^{(0)}))$
		- FFN … feed forward network (multi-layer perceptron)
		- MATL … multi-head attention layer
		- $E^{(0)}$ … embeddings with positional encoding
	- similarly for $E^{(\ell)}=\mathrm{Norm}(\mathrm{Norm}(E^{(\ell-1)}+\mathrm{MATL}^{(\ell)}(E^{(\ell-1)}))$ $+\ \mathrm{FFN}(\mathrm{Norm}(E^{(\ell-1)})+\mathrm{MATL}^{(\ell)}(E^{(\ell-1)})))$
- example
	- input text: Machine learning is learning from
	- tokenization: machine learn ing is learn ing from
	- embeddings: $E^{(0)}_1,E^{(0)}_2,E^{(0)}_3,E^{(0)}_4,E^{(0)}_5,E^{(0)}_6,E^{(0)}_7$
		- where $E^{(0)}_2=E^{(0)}_5$ and $E^{(0)}_3=E^{(0)}_6$
		- $|V|$ … vocabulary size (number of possible tokens)
			- 50 257 tokens in ChatGPT 2
		- $d_{\mathrm{model}}$ … dimension of the context independent vector representing a token
			- 12 288
		- $W_E$ (embedding matrix) has $|V|$ columns and $d_\mathrm{model}$ rows, is initialized randomly
	- adding positional information
	- MATL … multi-head attention layer
		- attention … $\mathrm{softmax}(\frac{K^TQ}{\sqrt{d_k}})\cdot V$
			- $K=W_kE$
			- $Q = W_qE$
			- $V=W_vE$
		- attention helps to distinguish the meaning based on the context
		- sometimes we need to use masking (in a decoder – otherwise, it could just read the next token from the target output)
		- $\Delta E_i^{(\ell-1)}=W_o(\sum_{j=1}^i\sigma (K_j\cdot Q_i)V_j)$
		- dimensions of the weight matrices ($W_k,W_q,W_v$) … 128 × 12 288
			- 128 … size of the hidden space
				- output vector of the attention mechanism has size 128
			- we have 96 attention heads
				- so just by concatenating the outputs, we get $128 \cdot 96 = 12\,288$
		- to get the output of MATL, we just concatenate the outputs of individual heads and multiply it by $W_o$
			- dimensions of $W_o$ are 12 288 × 12 288
	- Norm … just normalize the matrices (subtract mean, divide by variance)
	- parameter check
		- embedding
			- $W_E$ … 12 288 × 50 257
			- $W_U$ … 12 288 × 50 257
				- unembedding matrix, computes the distance to the individual words (then we select the closest one)
		- attention
			- $W_q$ … 128 × 12 288 × 96 heads × 96 layers
				- same for $W_k,W_v$
			- $W_o$ … 12 288 × 12 288 × 96 layers
		- MLP
			- 115 (?) bilion parameters
	- MLP – projection to a vector space with 4 times more dimensions (and back)
		- ReLU activation